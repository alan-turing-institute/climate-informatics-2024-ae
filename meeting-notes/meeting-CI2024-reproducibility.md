---
tags: meeting
description: Organising for Climate Informatics 2024 Reproducibility
---

:::info
:::spoiler **Expand for Contents**
[TOC]
:::

---
:::spoiler Expand for formatting template
```
<!-- %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% -->
# Meeting yyyy-mm-dd
## Attendees
## Apologies
## Notes
<!-- %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% -->
```
:::

==Actions are highlighted yellow==

# Reproducibility Working group 

## Discussion at CI 2024-04-22 to 2024-04-24

### Reviewer volunteers collected at CI
- Amy Winder, JBA Consulting (invited to slack)
- Roxanne Upton, JBA Consulting (colleague with Amy)
- Andrew McDonald
- Alexandra Udaltsova, OpenClimateFix (6 week intern, currently) (find in conf registration)
- Vadim Bertrand, IGE, vadim.bertrand@univ-grenoble-alpes.fr (others from thier team may also be interested)
- Paolo Pelucchi, Universitat de València, PhD Student (reviewer in 2023 Reproducibility Challenge)


### Questions from potential reviewers
- How will reviewers be matched to artfacts
- What expertise what is required?
    - Quite good at python
    - Have used github
    - Data science, some ML and DL
    - Do you need domain expertise
- Scheduling: 
    - A break would be good - people will have emails to get through after CI
    - Could only have 3 days available (industry)
        - A bookdash style thing would be easier to justify the time away.

### Zenodo community
- Are we happy to use this for AE?
- Requirments/resourcing
    - Who to moderate submissions?
    - Add instuctions to our repo


## Related resources
* Issue in the 2024 Climate Informatics repository, https://github.com/alan-turing-institute/climate-informatics-2024/issues/12
* Proposal document, https://hackmd.io/@dorchard/BkdTYf02p
* Test website in HotCRP, https://test4.hotcrp.com/
* Panel Planning odc
    * https://docs.google.com/document/d/12vQVvN5p_9e4lPDWW02cpRb_lhuHBuBwHxuWcV6fBrM/edit#heading=h.5p2qlhzh9fr9

## Members
- Alejandro Coca-Castro, The Alan Turing Institute
- Andrew Hyde, Cambridge University Press
- Cassandra Gould Van Praag, The Alan Turing Institute
- Douglas Rao, Climate Informatics / North Carolina State University
- Dominic Orchard, ICCS
- Lelle
- Marion Weinzierl, ICCS 
- Roly Perera, ICCS

# Meeting 2024-05-17

## Attendees
- Roly
- Doug
- Cass
- Dom
- Alejandro
- Andrew
- Aoife (TTW RCM, JOSS editor)

## Apologies
- Marion

## Notes
- Welcome Aoife!

- Reviewers in addemdun

- Informative session
    - Participation
        - 10 authors
- [Please register here!](https://turing-uk.zoom.us/meeting/register/tJAqf-msqDwvG93hDTG9y8VAiz4dwahkFCz7#/registration)
- Badges
    - They will be a figure in the adendum
    - Should be nice and visible
    - Include some wording about the criteria alongside
    - Are ACM ones CC-0?
    - [badges here](https://www.acm.org/publications/policies/artifact-review-and-badging-current)
    - They should be version controlled
    - ==Andrew can arrange for CUP marketing team to do some work on them==
        - Prefer this to come from CUP to legitmise and support adoption in the journal!
- [Slides](https://docs.google.com/presentation/d/1VlP7IkRVb22Q_wWjeuSW0tvl6TC-3tEJk0TacHHw1x8/edit#slide=id.g2db425f2bf0_0_11)
    - Whole presentation 20 min, the 40 min Q&A
    - Q&A not recorded
    - ==Note questions in a HackMd which can be turned into an FAQ==
    - Presenter order:
        - Alejandro: slides 1-2 (screen sharing)
        - Roly: 3-4
        - Doug: 6
- **Artefact submission deadline**
    - Do we want people to register their interest before submitting?
    - Platform requires a "draft submission" (e.g. name)
        - On publication, ask them to enter basic information (e.g. name) on HotCRP
        - Papers accepted notification in June
        - ==Use June to September to recruit reviewers==
        - Andrew's team to push for authors to reigster on HotCRP 
    - **AE submission deadline 12th September**
- Meeting slot
    - Changing to monthly
    - GitHub
        - [AE repo](https://github.com/alan-turing-institute/climate-informatics-2024-ae)
            - Add:
                - governance
                - Ways of Working
            - ==Roly to simplify the CI project board==

- Aoife's side project
    - LLM analysis of github repos
    - Run's locally
    - Suggestion to run this 

# Meeting 2024-05-10

## Attendees
- Roly
- Marion
- Douglas
- Andrew

## Apologies
- Cass
- Dom

## Notes
- Send slots to Andrew
    - Monday 20th, 11am-12pm
    - Record for other authors/reviewers
- Slides
- Final number of full papers
    - Roly, how is visible in EDS journal?
    - Versioned badges/docs
- Introductory
    - Ok for Douglas and Andrew H
    - Logistics
      - Zoom registration
- AE process
    - At least 2 of us per week
    - Marion: RSE
    - Doug: Sep 16-20 for NOAA AI Workshop
    - Andrew H: conference 23
    - Roly: first 2 weeks as well
    - Marion: October 15/16
- AOB

- ==Alejandro, Zoom registration==
- ==Form for reviewers==

Andrew, Friday, 11am-12pm

# Meeting 2024-05-02
## Attendees
- Marion
- Cass
- Roly
- Alejandro

## Apologies
- Dom
- Douglas 

## Notes
### Format
- Industry folks suggest a short format, 3 days
    - Would also need to find money for this
- Long format would align better with normal research practice
- If code take 6h to run, won't get much out in 3 days
- Hackaton-type atmoshphere would be positive
    - Different activity with different benefits
- RSE-con in Septmeber
- Clear that there is a lot of reproducibility ground work which needs to happen in this community
- ICCS might be interested in hosting
    - Also very TTW aligned
- Maybe long format is good for next year (ci-2025) and bundle this into a longer offering

### Informative session
- "May"
- Contents
    - What is an artefact?
    - What are badges?
    - HotCRP
    - ==Alejandro to draft slides for review next week==
- When
    - 1h Q&A
    - Authors and reviewers
    - ==[Please fill out availablity](https://whenisgood.net/593g3rn)==
        - Record and distrubute
    - ==Cass will set up zoom meeting and registration==

- https://neuromatch.io/climate-science/

- Env impact
    - UKRI recomending a Green RSE in every group
    - Marion approached by Kirsty Pringle - putting on a session at RSE Con. More traction in HPC field
    - There have been smaller scale conferences focusing on the env impact of computations, green alorythms.
    - Neural networks not well discussed, especially in training phase.

# Meeting 2024-04-19
## Attendees
- Roly
- Alejandro
- Lelle
- Cass
- Marion
- Dominic

## Apologies

## Notes

Updates:
* Dominic clarified review elements and PR, [pull request is up](https://github.com/alan-turing-institute/climate-informatics-2024-ae/pull/8), Roly and Cass will take a look into that next week,
* Dominic added checklist of remaining points in issues. 
    *    Do we want to an open or closed process?
        Alejandro: Iterations of feedback could be open, but in a single channel of communication/platform
Roly: Markdown submission, how claim was supported by the artefact, then reviewers provided feedback, and the review is this ongoing interaction of resubmissions, after some period of time (one month?) the author is 'logged off' and then reviewers provide final review/comments. 'Constructive interactive process with authors'.
In HotCRP, the review can be transpareent among reviewers, and it can be **collaborative**.
* Alejandro: Badges instead of nubmers
* Roly: Normally you rank papers, but in this case, we review wheather the submissions meet the criteria
* ==Test site URL is not working== >Test sites deleted after two weeks
* Platform cost $7.5 per submission. ==ICCS can cover it with credit card== 
* [Microwebsite discussion issue](https://github.com/alan-turing-institute/climate-informatics-2024-ae/issues/7) For Reproducibility Challenge last year, there was a Jupyter Notebook. Cass: CI template could be reused, signposting to external links. OR even: inside the CI website, a new page to be consistent that it's part of the conference ==Cass will update issue==
* Timeline: A month for evaluation . September - October (after proceedings publication)
    * July / August slower months, academic activities pick up in September / and REG availability better then
    * Comms to authors timeline
    (AH: In 2023: Conference was 21-22 April; I contacted authors on 3 May with option to submit to EDS and instructions to upload paper with a 29 May deadline; articles were accepted in early June and published first week of July (i.e. approx. 2.5 months between conference and publication.)
* June/July instead of September to coincide with paper-writing. Can we start as early as 'acceptance is confirmed'? Can this be pushed earlier to late May?
    * Paper acceptance 1st June
    * 1 month to prepare artefact for submission
    * 5 weeks to review
    * 
* 

#### Panel
* Announcement of the AE
(Chair will promote at the end of the panel as part of closing comments)
* [Panel structure](https://docs.google.com/document/d/12vQVvN5p_9e4lPDWW02cpRb_lhuHBuBwHxuWcV6fBrM/edit)
* ==Speakers to share their bios + speakers== 
    * [Dominic](https://dorchard.github.io/bio) will add his in the markdown
* Slides - 5 minutes lightning introductions on who you are, what you are working on, in context - a timeline of how you've been working on reproducibility (e.g Alejandro will present the position paper)
    * 1 slide - timeline of getting into repro work
    * 1 slide - problem/question currently thinking about
    * 1 slide - one project
* ==Cass: Share questions with ethics panel - Doug to connect==




# Meeting 2024-04-18

## Notes
- Important to provide link to the website, not generic login supported
- potential website name
    - ci2024_ae

- Submission
 - Set-up

- Admin
    - Authors/Reviewes interaction
    - Enable reviewing, last week
    - Anonymise reviewers
    - Discussion lead for controversial submission
        - Central point of contact for AE
   - Set tags at the end of the reviewers
   

- Features
    - Review
        - Ranking doesn't make for us
    - Review form
        - Change to choices in Overall merit
    - Tags & tracks
        - for notification templates templates
        - Read-only tags
            - change to 
                - reject available functional reusable
        - badges
            - green: available
            - pink: functional
            - red: reusable
    - Decisions
        - Enable conversations
        - Decision visibility
            - authors see decisions
            - reviewers see decisions
        - Decision types
            - Functional
            - Available

- Questions for the group meeting
    - Decisions
        - Collect final versions of accepted submissions 
    - What the long process will look like?
    - The goal of the process
        - single shared (collaborative) aggreement, little divergence
    

# Meeting 2024-04-12
## Attendees
- Alejandro
- Cass
- Roly
- Andrew

## Apologies
- Marion
- Dom

## Notes
- [AE Repository set up](https://github.com/alan-turing-institute/climate-informatics-2024-ae)
- [issue #3 - criteria names](https://github.com/alan-turing-institute/climate-informatics-2024-ae/issues/3)
    - Keep these aligned with 
- [issue #2 - evaluation committee (reviewers)](https://github.com/alan-turing-institute/climate-informatics-2024-ae/issues/2)
    - Cass to make sure we're reaching outside REG and Cambridge, e.g. into Turing E&S GC channels
    - We haven't communicated with authors how they can be part of the review process
         - Make sure to invite authors explicitly - abstracts and full papers
    - Invite to EDS Journal reviewers 
    - At invitation, open an expression of interest for futhre AE exercises.
    - Issue updated to add the above suggestions
    - ==Need to clarify minimum skill/expereince requirements for reviewers==

- [issue #4 - platforms](https://github.com/alan-turing-institute/climate-informatics-2024-ae/issues/4)

- [issue #6 - support](https://github.com/alan-turing-institute/climate-informatics-2024-ae/issues/6)
    - Needs to be post meeting. We cannot support even informal gatherings which are exclusive to some participants and distracts from the main body of the meeting.
    - Advertise this during the repro panel, as an online session after the meeting
    - We can use HyHive for 2 weeks after the conference - this could be our deadline for 



- [zenodo community for Climate informatics](https://zenodo.org/communities/climate-informatics/) :heart_eyes_cat: 
    - Q: Who is reviewing submissions to this? 




- Panel
    - No response from Lisa, so move to Thuy-Vy Nguyen
        - Cass sending email today
    - [Format](https://docs.google.com/document/d/12vQVvN5p_9e4lPDWW02cpRb_lhuHBuBwHxuWcV6fBrM/edit#heading=h.5p2qlhzh9fr9)
    - Open all questions to all speakers
    - ==Send questions in advance to speakers==


# Meeting 2024-04-03
## Attendees
- Roly
- Alejandro

## Apologies

## HotCRP

### System admin (to set before)
- Users
- Topics
- Deadline
- Review Scores
    - Overall merit
    - Reviewer familiarity

#### Users
- Add accounts ok, but
    - require user to sign in manually
    - invite in the SPAM folder (Alejandro's gmail account)
    - Problems with inviting existing accounts (Roly)
- Manually confirm with program committe

### Submission (Authors) 
- Do we need separate author and reviewer checklists? (Probably just latter.)
- Checklist
    - Add README or artifact-evaluation.md (example [here](https://github.com/explorable-viz/fluid/blob/c1012058407af3080448d972bc95bef24a720daf/artifact-evaluation.md))
    - Create a branch 

### Review
- Initial dummy review to enable interaction with authors (15 days)
    - PC chair notifies reviewers of a hard deadline to submit a draft reviews and began discussion about what badges to award
- Actual review comes towards the end of the process (after author interaction) (5 days)
- Authors submit revised artifact (ideally) by pushing a new revision to repo (or updating the existing one)
- Review Scores
    - Overall merit: available, functional, reusable
    - Reviewer expertise: some familiarity

## Badges
- Not in HotCRP
- Zenodo, doi, not guarantee


# Meeting 2024-03-28
## Attendees
- Andrew H
- Marion
- Cass
- Dom
- Roly
- Alejandro

## Apologies
- Douglas

## Notes
### Award criteria
- Are we agreed that the outcome criteria decisions are appropriate?
    - Levels
        - **Available**
            - > Author-created artifacts relevant to this paper have been placed on a publically accessible archival repository. A DOI or link to this repository along with a unique identifier for the object is provided.
                - > We do not mandate the use of specific repositories. Publisher repositories (such as the ACM Digital Library), institutional repositories, or open commercial repositories (e.g., figshare or Dryad) are acceptable. In all cases, repositories used to archive data should have a declared plan to enable permanent accessibility. Personal web pages are not acceptable for this purpose.
                - > Artifacts do not need to have been formally evaluated in order for an article to receive this badge. In addition, they need not be complete in the sense described above. They simply need to be relevant to the study and add value beyond the text in the article. Such artifacts could be something as simple as the data from which the figures are drawn, or as complex as a complete software system under study.
        - **Functional**
            - > The artifacts associated with the research are found to be documented, consistent, complete, exercisable, and include appropriate evidence of verification and validation.
                - > Documented: At minimum, an inventory of artifacts is included, and sufficient description provided to enable the artifacts to be exercised.
                - > Consistent: The artifacts are relevant to the associated paper, and contribute in some inherent way to the generation of its main results.
                - > Complete: To the extent possible, all components relevant to the paper in question are included. (Proprietary artifacts need not be included. If they are required to exercise the package then this should be documented, along with instructions on how to obtain them. Proxies for proprietary data should be included so as to demonstrate the analysis.)
                - > Exercisable: Included scripts and/or software used to generate the results in the associated paper can be successfully executed, and included data can be accessed and appropriately manipulated.
        - **Reusable**
            - >The artifacts associated with the paper are of a quality that significantly exceeds minimal functionality. That is, they have all the qualities of the Artifacts Evaluated – Functional level, but, in addition, they are very carefully documented and well-structured to the extent that reuse and repurposing is facilitated. In particular, norms and standards of the research community for artifacts of this type are strictly adhered to. 
        - ~~Reproduced~~
        - ~~Replicated~~
    - [Based on ACM](https://www.acm.org/publications/policies/artifact-review-and-badging-current)
        - Good for authors to have a clear descripion of the cirteria published, maybe a visual?
- COS badges?
    - Note we cann't award ACM badges directly due to complexities of integrating with CUP processes,
    - adendum after publication that the artefacts have been through an extended reproducilbility review, as described in the position paper we are preparing (a pre-print at this stage would be nice).
    - Publish the critera and process in our github repo, so it can be referenced in the adendum
    - Suggestion that we should point towards existing published ACM cirteria rather than attempt to publish our own 

- Is there a checklist for reviewers?
    - Not sure. Dom will look through the reviewer materials
    - [Recent author guidance](https://2024.splashcon.org/track/splash-2024-oopsla-artifacts#Call-for-Artifacts)
    - ==Marion and Dominic could come up with a checklist==


### Blinding
- Open conversations
    - Github issues is suitable?
    - Someone should test this
    - Would EDS book be appropriate? If yes, go with this system!
        - May need to be slightly changed, with a updated issue and checklist
    - ==Alejandro to check==


- Closed conversations 
    - via [Linklings (paid)](https://web.linklings.com)
        - ==Look into costing? Marion?==
    - Artefacts themselves will still be open (and versioned)
    - only review will be closed 
    - versioning (with github / gitlab zenodo)
        - One version at the point of submission
        - Expect an updated version after the review process
    - CUP
        - May need to be modified
    - HotCRP
        - [HotCRP](https://hotcrp.com)
        - Used already by Dom, Roly
        - Email based workflow
        - ==Alejandro to look with Roly==
    - Need to make sure the open and closed processes dont diverge
    

### Timelines
- Panel at the conference
- Practical introduction for authors and reviewers
    - > There will be a 1 hour session for authors who are considering submitting artefacts to give an overview of the approach, some lightning talks on producing reproducible artefacts, discussion of the criteria, and an opportunity to ask questions about the process.
    - "Reproducible Artefact Evaluation 101"
        - Criteria
        - Processes/Tools
- post conference 6 weeks
- **We need to run training for reviewers, otherwise this will fail**
- Full papers expected to be published 4-6 weeks after the conference
    - The reivew has already happened by PC, so author revisions will be minimal
- Expect our timeline for reivew to be 6 months?
    - Recruitment of reviewers!
    - Roly's last process took 5 weeks
    - What is our minimum requirement for reviewers?
        - Version control
        - command line
        - programming
        - Point to other resources, e.g. TTW if they don't have this skill level
    - Training will look like
        - **half day**
        - checklist
        - prelimiary reading
        - Detail of the process
        - discuss expectations, e.g. "wiggle room"
        - Reviewers clarifications incorporated into our documentation of the process
    - Training to be completed before we open the authors artefact submission
        - Send them the full revised documentation, after input of reviewers
    - ==Cass to manage==
    - How many reviewers
        - 16 papers
        - 2 for each submission, 2 submission each reviewer
        - => 16 reviewers
    - Lelle is going to be supporting the training and expenses etc.

### Pannel 
- expect expenses to be covered
- ==check if childcare costs can be covered==
- Need one speaker from outside our group
    - ask Lisa DeBruine fist, then Thuy-Vy
    - These would both be able to speak to this topic as a community movement









### Deffinitions
- Repeatability (Same team, same experimental setup)
    - The measurement can be obtained with stated precision by the same team using the same measurement procedure, the same measuring system, under the same operating conditions, in the same location on multiple trials. For computational experiments, this means that a researcher can reliably repeat her own computation.
- Reproducibility (Different team, same experimental setup)*
    - The measurement can be obtained with stated precision by a different team using the same measurement procedure, the same measuring system, under the same operating conditions, in the same or a different location on multiple trials. For computational experiments, this means that an independent group can obtain the same result using the author’s own artifacts.
- Replicability (Different team, different experimental setup)*
    - The measurement can be obtained with stated precision by a different team, a different measuring system, in a different location on multiple trials. For computational experiments, this means that an independent group can obtain the same result using artifacts which they develop completely independently.



<!-- %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% -->


# Meeting 2024-03-15

## Attendees
- 

## Apologies

## Notes
### Panel - artefact evaluation
- Speakers 
    - Marion
    - UKRN
    - Other suggestions?
    - Riot Science
    - https://www.damtp.cam.ac.uk/person/sje30
        - Stephen Eglen
        - Kirsty Pringle +1 (also Green RSE network) - Edinburg
    - Alejandro was on panel last year
    - Kiera from CUP on panel last year
        - Reality of the workflow
    - Think about including researcher perspective
    - REG
        - Turing Tomas Lazauskas Research Computing lead
    - The daunting practical stuff from the front line!
    - **Emma Hogan is SSE at the MetOffice, set up Community of Practice**
- What's the message that we're trying to get accross?
    - Resourceing for computational reproducibilty?
    - Bottom-up and top down representation
    - Riot science at Durham contact (Marion)
    - 
- ==Alejandro to write up into list of potentials, to be confirmed async==

### Review process
- [Dom's proposal](https://hackmd.io/vKnQW2nGSjWdtDvefZcpaw?both)
- Runs over 2 (maybe 3) phases
- A lot of knowledge transfer happens informally between reviewer and author
    - Missing documentation has to be updated (and resubmitted) rather then hand-holding through communication
- Could bring in a second reviewer?
- Encourage authors to push improvements through the process
    - Should aim for this as an expectation.
    - Improvements mmust be captured somewhere
        - Where? In an issue?
- Would review be anonymous?
    - If you start a proccess non-anonymously, you can't go back.
    - Make the decision at the end
    - Github would mean it is not double blind
    - These are already accepted papers, so will already know who the authors are
    - If you wan't to participate in the review process, expect
    - We need to support reviewers through the process to make sure they are not intimidated by the balance of power?
        - - Reviewers will be concerned about invoking the wrath! 
    - Dom fears people will be discouraged by exposing the failure
        - People have had their arefacts rejected through this process, and they were pleased that it wasn't public that they had tried and failed!
    - Are people going to fail?!
        - Everyone is going to get open data and material badges anyway
    - in the ACM process, can submit for the highest level of reproducibility, but only award the best level which was reached.
        - Dom got one: available, functional, but not reusable
            - **Can authors nominate the leve they are going for?**
    - **Perhaps this is something we could talk about in the panel?**
- Final output is a position paper where authors and reviewers are invited to co-author
- Re: anonymity - have to offer both open and closed to both reviewers and authors
    - If any one person wants it to be closed, i needs to be closed
        - But then how do reviewers get credit?
        - Will aim to work in all options
    - Want this to feel like a collaborative rather than competative "your job depends on it!" activity, cf. journal reejction! 
- Proposal needs detail on infrastructure
    - Where are people going to run the artefacts?
- 


## Async

### Andrew H - Publication workflow for the opt-in papers 
1. Authors of articles taking part in the challenge are asked to fulfil the author-disclosure criteria for awarding the Open Science badges before acceptance (i.e. the data and materials have to be in a repository with permanent identifier; the Data Availability Statement contains the identifier; for materials on GitHub this might mean archiving them in Zenodo using this tool so that they get a DOI).

1. Articles taking part in the challenge are accepted for publication. Upon export we use the standard process for publishing articles with badges, which includes the standard wording in the footnote. (This article was awarded Open Data and Open Materials badges for transparent practices. See the Data Availability Statement for details).  Articles taking part in the challenge are published with badges.

1. Meanwhile, the reproducibility challenge is taking place: the reproducibility reviewers assess the artifacts and see if they can reproduce the results.

1. For those articles that ‘pass’ the reproducibility review, we publish an addendum which is linked to the original article. This notes the article was subject to and passed a reproducibility review and thanks the reproducibility reviewers.

1. In addition, reproducibility reviewers may be invited to publish a replication paper/reproducibility note as a separate article linked to the original article that details the process of assessing the article’s data and code. (This was suggested by one of the Chairs of the Conference).
 
Alongside 4) the publication of the addendum, if my colleagues consent, wed also update the original article, so that the footnote accompanying the badge reflects that the data and code have been peer reviewed. The acknowledgments section is also updated to credit the reproducibility reviewers using the CRedIT taxonomy.

<!-- %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% -->

<!-- %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% -->
# Meeting 2024-02-29

## Attendees
- Cass
- Alejandro
- Dominic
- Marion
- Roly
- Douglas

## Apologies
- Andrew

## Agenda
- Discuss CUP
- [Areas of Work](https://github.com/alan-turing-institute/climate-informatics-2024/issues/12#issuecomment-1946578421)
- AOB
    - VIP tickets, share promo-code and confirm dates 

## Notes
- Dominic outlined plan
- Doug wants to know what to communicate this to accepted papers
- Our plans are aligned with the "direction" of travel of CUP, but will be unlikely to change conditional processes of CUP at this time
- Would be valuable to document the processes and resources required to deliver the level of artefact review we're aiming for. 
    - Writing it up as an operational process
    - Gradual journey towards full adoption see over the last 9-10 years in computer science journals

- How are we acknowledging/rewarding if we can't offer badges?
    - Artefact published alongside the article (another doi)
        - ["More than a zenodo doi" examples](https://drops.dagstuhl.de/entities/document/10.4230/DARTS.8.2.1)
    - We could write something (a perspective/operations piece) and give them authorship. 
        - Potentially another special collection in EDS with mini data/software papers (short contribution from each author)
    - Expect them to get training etc.
    - Extra note explaining the extent of the testing

- [ACM Rep](https://acm-rep.github.io/2024/) for dissemination

### Review
- Dominic and Roly have experience with this platform:https://hotcrp.com/
    - Dominic is writing up a version of the process which could work for us
- Reviewer trianing
    - Marion doesn't have any 
    - Some on [ReproHack](https://www.reprohack.org/participant_guidelines)
    - Potentailly 20 reviewers, 2 artefacts each.
- Suggestion to prepare outline brief of "bestcase", "acceptable" and "please not this" versions of review training so we can scope resourcing.

### Governance
- Chair, what's reproducible? expectations e.g. benchmarks 

### Actions
==Dom to validate sponsorship via peer-review platform for assessing artefacts==

==Alejandro will share poll for the next meeting, 15 March 2024, any time, except 2pm==


## Async work

### Cambridge Core Platform 
1. Badges: What might seem like an apparently simple thing, adding a badge to a publication, is in fact a non-trivial issue. As you know we've been working with the Centre for Open Science badges (https://www.cos.io/initiatives/badges) and there is a workflow for including these on Cambridge journals that covers: a) the award of the badge in the peer review system; b) agreed XML for the display of the badges in the published article; c) the agreed rendering of these badges on Cambridge Core. I enquired with my colleague Jenny who set up the workflow and it's not straightforward to just add further badges into this workflow, like the ACM ones, without discussion and further development that we probably don't have time for. So I think as far as EDS is concerned, we're stuck with using the Center for Open Science Open Data and Open Materials badges. (I also enquired with Centre for Open Science and they don't have specific Open Data / Open Materials badges that distinguish between the peer review and author disclosure models for awarding the badges). So the question is how to do something to indicate that the CI articles in question have been through the more rigorous reproducibility review (as opposed to the author-disclosure model that we generally use in EDS). The only idea I have for this is that we add a footnote to the badges and use the Data Availability Statement in the article to explain that this article has been through a rigorous reproducibility review process, perhaps linking to a dedicated page about this process. I have uploaded the screenshots of how badges display in the HTML article below for reference.
2. Versioning: Another frustrating fact about our platform is there is no functionality for versioning articles. However, we do have a precedent for retrospectively updating a published article with a badge by using an addendum. See for example this addendum: https://doi.org/10.1017/dap.2022.5. In this case it was a mistake: the original article (https://doi.org/10.1017/dap.2021.38) wasn't awarded Open Data and Open Materials badges when it should have been (under the author-disclosure model) because the authors made replication materials available via the Open Science Framework. So we published the addendum and retrospectively updated the original article (and the Data Availability Statement) with the badges. As you'll remember, we had a discussion about not wanting to dis-incentivise the authors who opt into the reproducibility challenge but giving them a slower timeline to publication. For those papers that opt-in, we could publish them initially without the badges (so as not to delay the timeline to publication compared to those that don't opt-in) and at a later date - after the reproducibility review process - publish an addendum and retrospectively update the article with the badge(s). This is a bit of a messy solution, but the best one that I can think of.

How CoS badges typically display on Cambridge Core in the article, in the Table of Contents, and how they are referred to in the Data Availability Statement (related to point 1 above

![image](https://hackmd.io/_uploads/B1BSjkRhp.png)

![image](https://hackmd.io/_uploads/ryd8iJRhT.png)

![image](https://hackmd.io/_uploads/S14DikRn6.png)

### Credits
Just summarising some thoughts on crediting reviewers who are reviewing the artefacts:
1. CUP provides a gift in the form of books to X amount (depends upon the number of reviewers)
2. Thank you list published somewhere (e.g. on a page on Cambridge Core dedicated to the project)
3. 'Credit' in the article (acknowledgements section in the article thanks the reviewers for their role in assessing manuscript; validation in the CRedIT taxonomy)
4. This might be more questionable but perhaps the publication of a final summary of their input, as an open review accompanying the article. As I mentioned, EDS is transitioning to a transparent peer review model (all review reports published; reviewers have option of being named or remaining anonymous). I think Dominic had previously said that a completely open process for these reproducibility reviews would probably not be desirable: it may make the reviewers feel as though they cannot be as fully critical as they might be in a blinded process. But perhaps the offer of the publication of a final summary of their input (what they did to reproduce the results, the interactions they had with the reviewers) could be a good way of giving recognition and credit. The screenshot below is of an open review published alongside an article in another journal: the review has a DOI so is a citable object itself.
5. (The problem with 4 though is that it would delay the publication of the article - to credit the reviewer in the form of an open review, we'd need to send that open review to production alongside the article - whereas 'credit' in the form of 3) acknowledgement could be managed as part of the post-publication addendum process i mentioned above).

![image](https://hackmd.io/_uploads/Sk6c3b0np.png)


<!-- %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% -->


<!-- %%%%%%%%%%%%%%%%%%%%%%%%%%%% -->
# Meeting 2024-02-15
## Attendees
- Alejandro 
- Dominic 
- Roly 
- Andrew
- Marion

## Apologies
- Dom (only participating the first 15 mins)
- Cass

## Agenda
- CI2024 status
    - Submissions
    - Organisers and reproducibility slack channels 
    - VIP tickets for CI2024 
- Existing Approaches/Workflows
- Our proposal for CI2024
- Resources

## Notes
- CI2024 status
    - 89 submissions (19 full papers, 69 abstracts, 1 panel proposal), 48 volunteer reviewers

    - CI organisers and reproducibility slack channels 

    - VIP tickets for CI2024, please confirm if online/in-person (how many days)
        - Marion, 3 days
        - Dom, 1 day
        - Andrew, 3 days
        - Roly, 1 day

- Approaches/Workflows
    - Computational notebooks
        - Private
            - Code Ocean (IEEE)
            - CoCal (CUP)
        - Community-driven initiatives
            - EDS book
    - Checklists
        - SC
            - conversation with the author
        - AGILE
        - ReproHack
            - Marion showing how it works. 
                - [10:14] Dominic Orchard
    this looks like a very useful interface actually. is it configurable at all? (I can imagine having a slightly different set of questions for the artefact evaluation)
                - Roly, asking about the process
                -https://www.reprohack.org/author_guidelines
                - 1 day event so not a lot of conversations in the process
                - The interface isn't customisable
    - Badges
        - ACM
        - Open Science Badging

- Unsuccesful approaches in Remote Sensing, see details in [Kredon and Frasier, 2022](https://doi.org/10.3390/rs14215471)
        - **Badging**, [Frery et al., 2020](https://doi.org/10.1109/JSTARS.2020.3019418) developed a badging system for four journals of the IEEE Geoscience and Remote Sensing Society. While the badging system may indeed work well for computationally intensive workflows, it will likely be difficult to apply the same system to mixed methods research in which remote sensing forms only a portion of the larger workflow.
        - **Remote Sensing Code Library**: https://tools.grss-ieee.org/rscl1/index.html, collects and shares code used in remote sensing research and si. However, that library is not currently accepting new submissions.

## Our Proposal (areas of work)

### Checklists
- [name=Marion] Clear expectations and time-boxing
    - Review guidelines
    - [name=Roly] Typical review workflow

### Commmunication
- [name=Roly] Conversations between author and reviewers
- [name=Alejandro]: Code of conduct

### Computational artifacts
- [name=Roly] Computational Artifacts (categories), examples and templates

Activities (awareness)
- [name=Andrew] Activities to help CI community, capacity-building
    - [name=Roly]: a panel, focus on computational artifacts and provenance
        - [name=Marion]: lightning talk (2 slides)
        - [name=Roly]: followed by a workshop, position paper
        - [name=Andrew]: cultural change
- [name=Marion] Blog post summarising Reproducibility Initiatives

Discussion on badges
- [name=Roly], badging (as part of the publication, or post-publication)
    - [name=Andrew]: opt-in
- [name=Alejandro]: Open science badges in EDS
    - [name=Andrew]: Open badges adopted since the start of EDS. Disclosure and peer-review.
- [name=Marion]: permissions to use ACM badges? https://www.acm.org/publications/policies/artifact-review-badging
- [name=Marion]: codecheck

Platforms
- [name=Marion]: e-mail, slack channel, google form

Reviewers
- [name=Andrew]: recruiting from the CI2023 reviewers
- [name=Alejandro]: sell 
- [name=Marion]: at least 2 reviewers
- [name=Roly]: from the submitted papers, how many will be accepted?
    - [name=Andrew]: less than 19 papers
    - [name=Roly]: ~10 papers with artifacts

## Resources
- Zotero public (closed membership) group on Reproducible Research and Geoscience (and other domains), https://www.zotero.org/groups/5376176/reproducible_research_in_geosciences
- AEC Submitters, https://docs.google.com/document/d/1pqzPtLVIvwLwJsZwCb2r7yzWMaifudHe1Xvn42T4CcA/edit

## Actions
- [name=Alejandro]: 
    - [x] Add members to the ci-2024-organisers channel
    - [ ] Set next meeting, 2 weeks, afternoon slots
    - [ ] Summarise conversations and next steps in [issue#12](https://github.com/alan-turing-institute/climate-informatics-2024/issues/12)

# Meeting 2024-02-02

## Attendees
- Alejandro (Turing E&S, EDS Book)
- Cass (Turing, Research Community Manager for Environment and Sustainability)
- Dominic Orchard (ICCS, Cambridge)
- Roly Perera (postdoc RSE at ICCS, ex Turing TPS funded, TTW slack)
- Andrew Hyde (CUP)
- Marion Weinzierl (ICCS Senior RSE)

## Apologies


## Notes
- ICCS RSE supports climate science teams
    - Currently setting up a repro hack 
- CUP gave prizes (books) to 2023 repro challenge
    - EDS Journal adopting open science badges
- papers in 2023 meeting n=15, 20 extended abstracts
    - Scale of research artefacts
- Final versions of the papers in EDS Journal after the conference
    - Articals not changed in any substantial way after the conference
    - Accepted on the basis of the peer review of the conference
    - Would be good to contact the authors early after papers accepted (for conferece) to give them more time to prepare.
- No centralised, consistent approach for awarding badges at CUP
    - Code Ocean
        - Some CUP journals use this
        - data analyst on editorial board reviews as a publication criteria
    - CoCalc
    - Each of the journals has their own governance
- We would like to be platform agnostic to how we accept artefacts
    - Don't expect there to be any barriers to proposal to do something in a new way
- Reproducibilty of the software side of the papers
    - Artefact evaluation process which happens in various areas of computer science
    - For the papers that are accepted, they can additionally submit artefacts 
        - Small committee review
        - with guidance, back-forwards interaction
            - Cass: Would be great to offer this as a supported process
        - award badges
            - Available (doi), functional (get it to work), others
- CUP uses [COS bages](https://www.cos.io/initiatives/badges)
    - awarded on the basis of an author disclosure 
    - Other badges and processes available, some may match ACM ones?
    - But good that CUP knows how to handle these badges in their publications
- [ACM has badges](https://www.acm.org/publications/policies/artifact-review-and-badging-current)
- Scope to change the timelines of paper/aretfact review in time for publication in CUP
- Who from the conference can we invite to review artefacts?
    - Marion was a reviewer for supercomputing badge
    - Lots of giving feedback
    - Had to submit artefact description
        - repo
        - doi
        - data store
        - how to run
    - Longer form with details on dependencies etc. 
        - Varying levels of details
    - 2 reviewers per submission
        - Have to be able to run it our cluster
        - confidential discusssions
    - Reviewers confer to decide level of award
    - Downstream nominations for awards
    - Process has been revised recently - was very workload heavy
        - "If you need to spend more than xh trying to get it to run, stop"
    - "Artefact description" (AD) level similar to COS badges
    - "Artefact Evaluation" AE
- ==Could deliver some training in reviewing after the conference==
- EDS Book badges
- A recent conference that Dominic's students and I had an artefact in: https://2024.splashcon.org/track/splash-2024-oopsla-artifacts
- Centre for Open Science outlines two processes for awarding: disclosure; and peer review: https://osf.io/tvyxz/wiki/2.%20Awarding%20Badges/
    - CUP uses the disclosure model (i.e. onus on author, just an admin check on our side to make sure that the data or materials are in a repository). If we wanted to stick with the Centre for Open Science badges, we could adopt the more stringent peer review criteria for awarding.
- Some useful guideline for artifact submitters (written for a CS audience): [How-to Guide](https://docs.google.com/document/d/1pqzPtLVIvwLwJsZwCb2r7yzWMaifudHe1Xvn42T4CcA/edit)
- Proposal
    - AE of some form
        - Minimally COS level, but aim for better!
    - interactive, structured rounds of feedback
    - VM or docker image
        - Maybe not well known/understood in the CI community
    - Artefact must be able to run on a laptop. If too big, have to minimally be able to assess that it's functional
    - Not blind, which is ok after paper accepting
    - Make sure it is valuable to the reviewers
        - Training
        - [CRediT](https://casrai.org/credit/) used by CUP: "Validation"
    - Need an AE committee of 20 people
        - Have to have enough background
        - 2nd year PhD student +
        - Turing REG
        - Good ECR devleopment opportunity
    - Panel on this at CI2024?
    - Published as an when ready
        - Not a special issue with a release date
        - 6 months after the conferenec to get to publication?
    - Managed dialogue between the reivewers and authors
        - CUP peer review system is optionally open
            - Inform the reviewers about opt-out disclosure of identity
            - $7.50 https://hotcrp.com/ used by ICCS

- ==Actions==
    - Async write up of procedure/process
    - Run activities after the conference
    - Session at the conference for visibility
    - Meet again in 2 weeks ish
        - Alejandro will look at calendars


# Meeting 2023-12-08

## Attendees
- Marion
- Alejandro

## Apologies
- Roly
- Dom

## Notes

### Recording
- Available in CI2024 Google Drive Folder. Access [here](https://drive.google.com/file/d/1Nl2imqVsg7iZu3656KNeVtyna4eQK6WC/view?usp=sharing).

### Intro
- Marion: ICCS, previous experience in Supercomputing reproducibility, organising/member of ReproHack
    - Chat with Codecheck initiative, 
-  Alejandro Coca-Castro

### Overview
- Climate Informatics
- Reproducibility Challenge
- Supercomputing - Reproducibility
    - https://sc23.supercomputing.org/program/papers/reproducibility-initiative/
    - Gradual changes
    - Feedback by committe members
    - Awards from 2021
    - Resources
        - Team of reviewers
    - Bottlenecks
        - Computing resources
            - Local
            - Cloud
            - Custom environment
    - Form for artifacts
    - Single-blind
        - Not open 

- Reprohack
    - https://www.reprohack.org/resources
    - https://www.reprohack.org/review/
    - Team-oriented (diverse)
    - Learn and feedback

- Codecheck
    - Authors/Reviewers collaborative process

- Practises
    - Supercomputing: gradual, extended, optional > then mandatory
    - Codecheck (optimal): feedback
    - Reprohack: community-oriented, over year

- Others
    - [ACM badges](https://www.acm.org/publications/artifacts)
        - Used by supercomputing

### Actions
==- [name=Alejandro]: Send calendar invite==

<!-- %%%%%%%%%%%%%%%%%%%%%%%%%%%% -->
