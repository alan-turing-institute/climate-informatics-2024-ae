# Climate Informatics 2024 Artefact Evaluation

This repository describes the process for artefact evaluation (AE) at the
Climate Informatics 2024 conference.

Climate Informatics, like many other communities and fields, has software at its heart. Underlying most publications is a novel piece of software playing some critical role, e.g., embodying a model, processing or analysing data, or producing a visualisation. In order for such software artefacts to have the most impact, they should be available, functional, and reusable, such that other researchers can benefit from the work, verify the claims of the paper, and then build upon the software to do more great work. These ideals are summarised by the FAIR principles of data, which can be applied to software: research software should be Findable, Accessible, Interoperable, and Reusable. In order to help promote [FAIR software](https://www.nature.com/articles/s41597-022-01710-x), Climate Informatics is embarking, for the first time, on an _Artefact Evaluation phase_ for full paper submissions, after those submissions have been accepted for publication as the conference proceedings in [Environmental Data Science](https://www.cambridge.org/core/journals/environmental-data-science) following the traditional peer-review process. Artefact Evaluation provides an opportunity to embed the values of reproducibility into the publication process in a lightweight opt-in fashion, thus encouraging authors to make software available and the results of the paper reproducible.

All authors of accepted full papers will be invited to submit their research artefacts (software and associated data) for formal review or "artefact evaluation".

## Contents

* [process.md] Describes in detail the rationale, process, and evaluation criteria in place;
* [badges.md] Provides a checklist which will be used in reviewing and to assess whether the artefacts are available, functional, and reusable.
