# Welcome ðŸ‘‹

Welcome to the Climate Informatics 2024 Artifact Evaluation Initiative!

```{admonition} Key Dates
* __Artifact submission deadline (extended) for authors:__ Thursday 4 October 2024
* __Final decisions sent to authors:__ Friday 8 November 2024
```

## The Problem

Climate Informatics, like many other communities and fields, has software at its heart. Underlying most publications is a novel piece of software playing some critical role, e.g., embodying a model, processing or analysing data, or producing a visualisation. 

In order for such software artifacts to have the most impact, they should be **available, functional, and reusable**, such that other researchers can benefit from the work, verify the claims of the paper, and then build upon the software to do more great work. These ideals are summarised by the FAIR principles of data, which can be applied to software: research software should be Findable, Accessible, Interoperable, and Reusable ([FAIR](https://www.nature.com/articles/s41597-022-01710-x)). 

The practicalities of achieving FAIR software are non-trivial, and require resourcing for both authors and reviewers, as well as community consensus on requirements and standards. 

## The Solution

In order to help promote FAIR software in our community, Climate Informatics is embarking, for the first time, on an optional _Artifact Evaluation (AE) phase_ for accepted full paper submissions. Those submissions will published as the conference proceedings in [Environmental Data Science](https://www.cambridge.org/core/journals/environmental-data-science) following the traditional peer-review process. 

AE provides an opportunity to embed the values of reproducibility into the publication process in a lightweight opt-in fashion, thus encouraging authors to make software available and the results of the paper reproducible. Submitted artifacts will be assessed by a skilled team of reviewers, who will work with authors to help them develop and share their materials with the highest practical level of computational reproducibility. 

## What are we doing?

We have adopted the AE process of the [Association for Computing Machinery artifact Review and Badging Version 1.1](https://www.acm.org/publications/artifacts), and developed this to fit the specific context of the Climate Informatics community (with minimal changes to retain alignment with this well accepted standard - we do not need to re-invent the wheel!). We have worked with our partners at Cambridge University Press to deliver this process in a way which complements their publication workflow, and does not delay dissemination of the work. 

The next stage is to design and deliver training and resources to the authors so the AE is a positive learning experience, and recruit and train a committee of reviewers to undertake the work of evaluation, with careful awareness of the workload and incentives to contribute their labour to this effort. 

Take a look at published materials and decisions made to date:
- [Rationale](overview-rationale): Describes in detail the rationale, process, and evaluation criteria in place;
- [Evaluation guidelines](overview-evaluation): Provides a checklist which will be used in reviewing and assessing whether the artifacts are available, functional, and reusable.
- [Issues](https://github.com/alan-turing-institute/climate-informatics-2024-ae/issues) is where we are openly recording our decision making processes.

## What do we need?

We need authors who are keen to submit their artifacts for evaluation, and reviewers who would like to contribute to the growth of reproducibility in our community! 

We will be recruiting reviewers in the coming weeks, and working with them to deliver the training and support they need to undertake this work. Take a look at the [benefits for reviewers](https://github.com/alan-turing-institute/climate-informatics-2024-ae/blob/main/process.md#benefits-to-reviewers) to understand how participating as reviewers will be a valuable opportunity for you, and stay tuned on the [Turing Environment and Sustainability Slack](https://alan-turing-institute.github.io/climate-informatics-2024/contact/#slack) for invitations to join the review committee!

## Who are we?

This work is being led by the [Reproducibility working group](https://alan-turing-institute.github.io/climate-informatics-2024/team#reproducibility) of the Climate Informatics 2024 organisers. We are pleased to connect with you if you would like to participate in the leadership or delivery of this work!

## Programme Committee

We thank the program committee for being available to review the submitted papers!

* Alexandra Udaltsova (Open Climate Fix)
* Bryn Noel Ubald (British Antarctic Survey)
* Etienne Roesch (University of Reading)
* James Emberton (ICCS, University of Cambridge)
* James Robinson (Alan Turing Institute)

## Contact us

### Slack

Connect with us via [Turing Environment and Sustainability Slack](https://alan-turing-institute.github.io/climate-informatics-2024/contact/#slack) - tag or dm Alejandro Coca-Castro (CI2024 Reproducibility Chair), or Dominic Orchard (CI2024 Reproducibility Co-Chair)

### Email

- Alejandro Coca-Castro (CI2024 Reproducibility Chair): acoca@turing.ac.uk
- Dominic Orchard (CI2024 Reproducibility Co-Chair): dao29@cam.ac.uk

## Acknowledgements

The AE process is developed following [Association for Computing Machinery artifact Review and Badging Version 1.1](https://www.acm.org/publications/artifacts).

This repo and README follow the best practice for community participation of _The Turing Way_ {cite:ps}`a-ttw_2022`.

## References

```{bibliography}
:style: alpha
:keyprefix: a-
```