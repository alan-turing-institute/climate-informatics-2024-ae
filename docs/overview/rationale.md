(overview-rationale)=

# Rationale

Climate Informatics, like many other communities and fields, has software at its heart. Underlying most publications is a novel piece of software playing some critical role, e.g., embodying a model, processing or analysing data, or producing a visualisation. In order for such software artifacts to have the most impact, they should be available, functional, and reusable, such that other researchers can benefit from the work, verify the claims of the paper, and then build upon the software to do more great work. These ideals are summarised by the FAIR principles of data, which can be applied to software: research software should be Findable, Accessible, Interoperable, and Reusable. In order to help promote [FAIR software](https://www.nature.com/articles/s41597-022-01710-x), Climate Informatics is embarking, for the first time, on an _Artifact Evaluation phase_ for full paper submissions, after those submissions have been accented for publication as the conference proceedings in [Environmental Data Science](https://www.cambridge.org/core/journals/environmental-data-science) following the traditional peer-review process. Artifact Evaluation provides an opportunity to embed the values of reproducibility into the publication process in a lightweight opt-in fashion, thus encouraging authors to make software available and the results of the paper reproducible.

A committee of reviewers, the Artefact Evaluation Committee (AEC), will review the submitted artefacts against three [criteria](evaluation): is the software available? is it functional, and can it be used to reproduce the (central) claims or thesis of the paper?

## Publishing workflow

We want to avoid deincentivising authors who opt into the reproducibility challenge by having this lead to a slower timeline to publication. For those papers that opt-in, we will therefore publish them initially without the badges (so as not to delay the timeline to publication) and at a later date, after artifact evaluation, publish an addendum and retrospectively update the article with the badge(s).

In the CUP platform is there is no functionality for versioning articles. However, there is a precedent for retrospectively updating a published article with a badge by using an addendum. See for example https://doi.org/10.1017/dap.2022.5. In this case it was a mistake: the original article (https://doi.org/10.1017/dap.2021.38) wasnâ€™t awarded Open Data and Open Materials badges when it should have been. So we published the addendum and retrospectively updated the original article and Data Availability Statement with the badges.

## Retrospective report

The reproducibility chairs will produce a general report after the artefact evaluation process documenting the approach and reporting on experiences. Reviewers ==and authors== will be invited to co-author the report, providing their experiences. Any experiences will be anonymised however with respect to the artifacts.
