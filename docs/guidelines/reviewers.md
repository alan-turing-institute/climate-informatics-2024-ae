(guidelines-reviewers)=

# Guidelines for Reviewers

Reviewers will be recruited to the AEC from the [Climate Informatics community](http://www.climateinformatics.org) (join [here](https://groups.google.com/g/climate-informatics-news)) and [Turing Environment and Sustainability Grand Challenge community](https://cassgvp.kumu.io/alan-turing-institute-environment-and-sustainability) (join [here](https://forms.office.com/pages/responsepage.aspx?id=p_SVQ1XklU-Knx-672OE-ZmEJNLHTHVFkqQ97AaCfn9UMTZKT1IwTVhJRE82UjUzMVE2MThSOU5RMC4u)). Please join either (or both!) communities to receive updates on reviewer training opportunities.

## Requirements for reviewers
We require a minimum level of practical experience in climate data science to participate as a reviewer. This ensures that the review process can focus on technical detail rather than computational literacy. Reviewers will be required to evidence their experience through their own software publications. Full reviewer requirements will be published ==XXX==

## Expected workload

- Each reviewer will be assigned 2 submissions
- There will be a 2-hour on-boarding session to get everyone up to speed
- Each review can be expected to take up to 2 working days to complete, spread over the ~5 weeks of the review process, including time spent communicating with the submitting author using HotCRP. We will consider an artifact that takes longer than 2.5 working days to review to be “unreproducible” for the purposes of this review process.  
- Final reviews should be submitted by TBD (mid October)

## Benefits to reviewers
Reviewers will benefit from hands-on training in high fidelity computational reproducibility, which we anticipate will support their own development of reproducible research artifacts. Reviewers will be able to reference their contribution as evidence of leadership culture change towards reproducibility, and invited to co-author a retrospective report to be published in [Environmental Data Science](https://www.cambridge.org/core/journals/environmental-data-science) after the review process is complete. They will be supported by the CI Reproducibility team and AEC throughout, thereby strengthening their connections with this highly skilled team.

##  Evaluation process

Reviewers will asses the artifacts against a checklists provided for each ["badge" level to be awarded](https://github.com/alan-turing-institute/climate-informatics-2024-ae/blob/main/badges.md).

For each point, reviewers will be required to briefly note the evidence for this point, anything they have done to validate that point (e.g., what did you need to reproduce a claim), as well as the outcome (negative, neutral, or positive) and a brief reason for their judgment.

## Feedback to the authors

This process has been designed to be collaborative and iterative. During the first 3 weeks, reviewers will be able to feed-back to authors.
Authors will be encouraged to push improvements to the artifact whenever possible. For example, if essential documentation is missing, then rather than submitting a negative evaluation or providing only the reviewers with the required information, the authors should provide that information via a versioned updated to the artifact.